{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "985f0a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97a007a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of letters : 3335477\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "\n",
    "with open(\"Dataset/en_US.twitter.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    data = f.read()\n",
    "    \n",
    "print(\"No of letters :\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c93e010d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sky', 'is', 'blue', '.'],\n",
       " ['leaves', 'are', 'green'],\n",
       " ['roses', 'are', 'red', '.']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre Processing text data\n",
    "\n",
    "def pre_processing(data):\n",
    "    \n",
    "    # String to Sentences\n",
    "    sentences = data.split(\"\\n\")\n",
    "    sentences = [text.strip() for text in sentences]\n",
    "    sentences = [text for text in sentences if len(text) > 0]\n",
    "    \n",
    "    # Tokenize sentences into tokens\n",
    "    tokenized_sentences = [nltk.word_tokenize(text.lower()) for text in sentences]\n",
    "    \n",
    "    return tokenized_sentences\n",
    "\n",
    "pre_processing(\"Sky is blue.\\nLeaves are green\\nRoses are red.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74230b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test sets\n",
    "\n",
    "tokenized_data = pre_processing(data)\n",
    "\n",
    "random.seed(69)\n",
    "random.shuffle(tokenized_data)\n",
    "\n",
    "train_size = int(len(tokenized_data) * 0.8)\n",
    "train_data = tokenized_data[0:train_size]\n",
    "test_data = tokenized_data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea3dcec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sky': 1, 'is': 1, 'blue': 1, '.': 3, 'leaves': 1, 'are': 2, 'green': 1, 'roses': 1, 'red': 1}\n",
      "{'are', '.'}\n",
      "[['dogs', '<unk>'], ['<unk>', 'sleep']]\n"
     ]
    }
   ],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def count_words(tokenized_data):\n",
    "    \n",
    "    data = []\n",
    "    [data.extend(text_l) for text_l in tokenized_data]\n",
    "    \n",
    "    return dict(collections.Counter(data))\n",
    "\n",
    "def get_closed_vocab(tokenized_data, threshold):\n",
    "    \n",
    "    vocab = count_words(tokenized_data)\n",
    "    closed_vocab = set([word for word in vocab if vocab[word] >= threshold])\n",
    "    \n",
    "    return closed_vocab\n",
    "\n",
    "def replace_oov(tokenized_data, closed_vocab):\n",
    "    \n",
    "    replaced_tokenized_data = [[word if word in closed_vocab else \"<unk>\" for word in sentence] for sentence in tokenized_data]\n",
    "    return replaced_tokenized_data\n",
    "    \n",
    "print(count_words([['sky', 'is', 'blue', '.'], ['leaves', 'are', 'green', '.'], ['roses', 'are', 'red', '.']]))\n",
    "print(get_closed_vocab([['sky', 'is', 'blue', '.'], ['leaves', 'are', 'green', '.'], ['roses', 'are', 'red', '.']], 2))\n",
    "print(replace_oov([[\"dogs\", \"run\"], [\"cats\", \"sleep\"]], {\"dogs\", \"sleep\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7c6855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Processing tokenized_data\n",
    "\n",
    "def pre_processing(train_data, test_data, threshold):\n",
    "    \n",
    "    vocab = get_closed_vocab(train_data, threshold)\n",
    "    \n",
    "    train_data_replaced = replace_oov(train_data, vocab)\n",
    "    test_data_replaced = replace_oov(test_data, vocab)\n",
    "    \n",
    "    return train_data_replaced, test_data_replaced, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42cc8b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Processing on train and test split\n",
    "\n",
    "train_data_processed, test_data_processed, vocab = pre_processing(train_data, test_data, threshold=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305e59cc",
   "metadata": {},
   "source": [
    "### N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2616f411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions \n",
    "\n",
    "def get_n_gram(data, n, start_token = \"<s>\", end_token = \"<e>\"):\n",
    "    \n",
    "    pairs = []\n",
    "    \n",
    "    for sentence in data:\n",
    "        \n",
    "        sentence = n * [start_token] + sentence + [end_token]\n",
    "        \n",
    "        for idx in range(len(sentence)-n+1):\n",
    "            pairs.append(tuple(sentence[idx: idx+n]))\n",
    "            \n",
    "    return dict(collections.Counter(pairs))\n",
    "        \n",
    "# print(\"Uni-gram :\", get_n_gram([['i', 'like', 'a', 'cat'], ['this', 'dog', 'is', 'like', 'a', 'cat']], 1))\n",
    "# print(\"Bi-gram :\", get_n_gram([['i', 'like', 'a', 'cat'], ['this', 'dog', 'is', 'like', 'a', 'cat']], 2))\n",
    "\n",
    "\n",
    "def estimate_probability(word, previous_n_gram, n_gram, n_plus1_gram, vocabulary_size, k=1.0):\n",
    "    \n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    \n",
    "    string_tuple = previous_n_gram + (word, )\n",
    "    \n",
    "    numerator = n_plus1_gram.get(string_tuple, 0) + k\n",
    "    denominator = n_gram.get(previous_n_gram, 0) + k * vocabulary_size\n",
    "    \n",
    "    probability = numerator / denominator\n",
    "    \n",
    "    return probability\n",
    "\n",
    "def estimate_probabilities(previous_n_gram, n_gram, n_plus1_gram, vocab, k=1.0):\n",
    "    \n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    \n",
    "    vocab = vocab.union({\"<e>\", \"<unk>\"})\n",
    "    vocab_size = len(vocab)\n",
    "    \n",
    "    probabilities = {}\n",
    "    for word in vocab:\n",
    "        probabilities[word] = estimate_probability(word, previous_n_gram, n_gram, n_plus1_gram, vocab_size, k=k)\n",
    "    \n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45255967",
   "metadata": {},
   "source": [
    "### Count and probability matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d90724f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>like</th>\n",
       "      <th>i</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "      <th>a</th>\n",
       "      <th>this</th>\n",
       "      <th>cat</th>\n",
       "      <th>is</th>\n",
       "      <th>dog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(like,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is,)</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(cat,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(i,)</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(dog,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(this,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         like    i  <e>  <unk>    a  this  cat   is  dog\n",
       "(like,)   0.0  0.0  0.0    0.0  2.0   0.0  0.0  0.0  0.0\n",
       "(is,)     1.0  0.0  0.0    0.0  0.0   0.0  0.0  0.0  0.0\n",
       "(<s>,)    0.0  1.0  0.0    0.0  0.0   1.0  0.0  0.0  0.0\n",
       "(cat,)    0.0  0.0  2.0    0.0  0.0   0.0  0.0  0.0  0.0\n",
       "(i,)      1.0  0.0  0.0    0.0  0.0   0.0  0.0  0.0  0.0\n",
       "(dog,)    0.0  0.0  0.0    0.0  0.0   0.0  0.0  1.0  0.0\n",
       "(a,)      0.0  0.0  0.0    0.0  0.0   0.0  2.0  0.0  0.0\n",
       "(this,)   0.0  0.0  0.0    0.0  0.0   0.0  0.0  0.0  1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_count_matrix(n_plus1_gram, vocab):\n",
    "    \n",
    "    vocab = vocab.union({\"<e>\", \"<unk>\"})\n",
    "    \n",
    "    n_grams = list(set([val[0:-1] for val in n_plus1_gram.keys()]))\n",
    "    \n",
    "    row_index = {n_gram:i for i, n_gram in enumerate(n_grams)}\n",
    "    col_index = {word: i for i, word in enumerate(vocab)}\n",
    "    \n",
    "    count_matrix = np.zeros((len(n_grams), len(vocab)))\n",
    "    \n",
    "    for key, value in n_plus1_gram.items():\n",
    "        \n",
    "        n_gram = key[0: -1]\n",
    "        word = key[-1]\n",
    "        \n",
    "        if word not in vocab:\n",
    "            continue\n",
    "            \n",
    "        i = row_index[n_gram]\n",
    "        j = col_index[word]\n",
    "        count_matrix[i][j] = value\n",
    "        \n",
    "    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=list(vocab))\n",
    "    return count_matrix\n",
    "\n",
    "sentences = [['i', 'like', 'a', 'cat'], ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "words = set(sentences[0] + sentences[1])\n",
    "bigram = get_n_gram(sentences, 2)\n",
    "\n",
    "get_count_matrix(bigram, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aabc13da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>like</th>\n",
       "      <th>i</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "      <th>a</th>\n",
       "      <th>this</th>\n",
       "      <th>cat</th>\n",
       "      <th>is</th>\n",
       "      <th>dog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(like,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is,)</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(cat,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(i,)</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(dog,)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(this,)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             like         i       <e>     <unk>         a      this       cat  \\\n",
       "(like,)  0.090909  0.090909  0.090909  0.090909  0.272727  0.090909  0.090909   \n",
       "(is,)    0.200000  0.100000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
       "(<s>,)   0.090909  0.181818  0.090909  0.090909  0.090909  0.181818  0.090909   \n",
       "(cat,)   0.090909  0.090909  0.272727  0.090909  0.090909  0.090909  0.090909   \n",
       "(i,)     0.200000  0.100000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
       "(dog,)   0.100000  0.100000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
       "(a,)     0.090909  0.090909  0.090909  0.090909  0.090909  0.090909  0.272727   \n",
       "(this,)  0.100000  0.100000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
       "\n",
       "               is       dog  \n",
       "(like,)  0.090909  0.090909  \n",
       "(is,)    0.100000  0.100000  \n",
       "(<s>,)   0.090909  0.090909  \n",
       "(cat,)   0.090909  0.090909  \n",
       "(i,)     0.100000  0.100000  \n",
       "(dog,)   0.200000  0.100000  \n",
       "(a,)     0.090909  0.090909  \n",
       "(this,)  0.100000  0.200000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_probability_matrix(n_plus1_gram, vocab, k):\n",
    "    \n",
    "    count_matrix = get_count_matrix(n_plus1_gram, vocab)\n",
    "    count_matrix += k\n",
    "    \n",
    "    prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n",
    "    return prob_matrix\n",
    "\n",
    "sentences = [['i', 'like', 'a', 'cat'], ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "words = set(sentences[0] + sentences[1])\n",
    "bigram = get_n_gram(sentences, 2)\n",
    "\n",
    "get_probability_matrix(bigram, words, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af68a15d",
   "metadata": {},
   "source": [
    "### Auto Complete System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80f35b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_a_word(previous_tokens, n_gram, n_plus1_gram, vocab, k=1.0, start_with=None):\n",
    "    \n",
    "    n = len(list(n_gram.keys())[0]) \n",
    "    \n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    "    \n",
    "    probabilities = estimate_probabilities(previous_n_gram, n_gram, n_plus1_gram, vocab, k=k)\n",
    "    \n",
    "    suggestion = None\n",
    "    max_prob = 0\n",
    "    \n",
    "    for word, prob in probabilities.items():\n",
    "        \n",
    "        if start_with != None:\n",
    "            if not word.startswith(start_with):\n",
    "                continue\n",
    "        \n",
    "        if prob > max_prob:\n",
    "            suggestion = word            \n",
    "            max_prob = prob\n",
    "\n",
    "    return suggestion, max_prob\n",
    "\n",
    "# sentences = [['i', 'like', 'a', 'cat'], ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "# unique_words = set(sentences[0] + sentences[1])\n",
    "\n",
    "# unigram_counts = get_n_gram(sentences, 1)\n",
    "# bigram_counts = get_n_gram(sentences, 2)\n",
    "\n",
    "# previous_tokens = [\"i\", \"like\"]\n",
    "\n",
    "# suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0)\n",
    "# suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0, start_with=\"c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c896d48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Text : Hello World !\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('<e>', 0.21491718889883618)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Play ground \n",
    "\n",
    "previous_tokens = input(\"Enter Text : \").lower()\n",
    "previous_tokens = nltk.word_tokenize(previous_tokens)\n",
    "\n",
    "unigram = get_n_gram(train_data_processed, 1)\n",
    "bigram = get_n_gram(train_data_processed, 2)\n",
    "\n",
    "suggest_a_word(previous_tokens, unigram, bigram, vocab, k=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "502021c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_a_word(sentence, n_gram, n_plus1_gram, vocab, k=1.0, options=None):\n",
    "    \n",
    "    previous_tokens = sentence.lower()\n",
    "    previous_tokens = nltk.word_tokenize(previous_tokens)\n",
    "    \n",
    "    n = len(list(n_gram.keys())[0]) \n",
    "    \n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    "    \n",
    "    probabilities = estimate_probabilities(previous_n_gram, n_gram, n_plus1_gram, vocab, k=k)\n",
    "    \n",
    "    output = {word: probabilities.get(word, -1) for word in options}\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23d817b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'principle': 6.718172657037286e-05, 'principal': 6.718172657037286e-05}\n",
      "{'excepted': -1, 'accepted': 6.738544474393532e-05}\n",
      "{'lose': 6.419309282321222e-05, 'loose': 6.419309282321222e-05}\n",
      "{'later': 6.739906989283548e-05, 'latter': 6.739906989283548e-05}\n",
      "{'stationary': -1, 'stationery': -1}\n",
      "{'excepted': -1, 'accepted': 6.739906989283548e-05}\n",
      "{'later': 6.734006734006734e-05, 'latter': 6.734006734006734e-05}\n",
      "{'affects': 6.739906989283548e-05, 'effects': 6.739906989283548e-05}\n",
      "{'council': 6.739906989283548e-05, 'counsel': 6.739906989283548e-05}\n",
      "{'too': 6.739906989283548e-05, 'to': 6.739906989283548e-05}\n",
      "{'council': 6.739906989283548e-05, 'counsel': 6.739906989283548e-05}\n",
      "{'bear': 6.739906989283548e-05, 'bare': 6.739906989283548e-05}\n",
      "{'fur': -1, 'far': 6.691648822269808e-05}\n",
      "{'furthest': -1, ' farthest': -1}\n",
      "{'advice': 6.697923643670462e-05, 'advise': 6.697923643670462e-05}\n",
      "{'loose': 6.739452756436177e-05, 'lose': 6.739452756436177e-05}\n",
      "{'to': 0.0006722237160527024, 'too': 0.00013444474321054048}\n",
      "{'quite': 6.711859856366199e-05, 'quiet': 6.711859856366199e-05}\n",
      "{'heap': -1, 'hip': 6.739906989283548e-05}\n",
      "{'there': 6.624271330153684e-05, 'their': 6.624271330153684e-05}\n"
     ]
    }
   ],
   "source": [
    "bigram = get_n_gram(train_data_processed, 2)\n",
    "trigram = get_n_gram(train_data_processed, 3)\n",
    "\n",
    "\n",
    "questions = \"\"\"Mr Patrick is our new (principle/principal).\n",
    "The company (excepted/accepted) all the terms.\n",
    "Please don’t keep your dog on the (lose/loose).\n",
    "The (later/latter) is my best friend.\n",
    "I need some (stationary/stationery) products for my craftwork.\n",
    "The actor (excepted/accepted) the Oscar.\n",
    "I will call you (later/latter) in the evening.\n",
    "Covid (affects/effects) the lungs.\n",
    "The (council/counsel) of the ministers were sworn in yesterday.\n",
    "Robert (too/to) wants to accompany us to the park.\n",
    "Mia will (council/counsel) me about choosing fashion as my career.\n",
    "The (bear/bare) at the zoo was very playful.\n",
    "The sheep have a lot of (fur/far) that keeps them warm.\n",
    "The hot spring is at the (furthest/ farthest) corner of the street.\n",
    "Can you (advice/advise) me on how to study for exams?\n",
    "The team will (loose/lose) the match if they don’t play well.\n",
    "Can you go (to/too) the market for me?\n",
    "The teachers asked the students to keep (quite/quiet).\n",
    "The (heap/hip) of garbage should be cleaned immediately.\n",
    "This is (there/their) house. \"\"\"\n",
    "\n",
    "questions = questions.split(\"\\n\")\n",
    "for question in questions:\n",
    "    \n",
    "    question = question.replace(\")\", \"(\")\n",
    "    question = question.split(\"(\")\n",
    "    \n",
    "    print(choose_a_word(question[0], bigram, trigram, vocab, k=1.0, options = question[1].split(\"/\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735b52cc",
   "metadata": {},
   "source": [
    "### Perplexity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7009c08c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2247.9099690172125"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_perplexity(sentence, n_gram, n_plus1_gram, vocab, k=1.0):\n",
    "    \n",
    "    n = len(list(n_gram.keys())[0]) \n",
    "    \n",
    "    sentence = [\"<s>\"] * n + sentence + [\"<e>\"]\n",
    "    sentence = tuple(sentence)\n",
    "    \n",
    "    N = len(sentence)\n",
    "    \n",
    "    product_pi = 1.0\n",
    "    for i in range(n, N):\n",
    "\n",
    "        list_t = sentence[i-n:i]    \n",
    "        \n",
    "        word = sentence[i]\n",
    "        probability = estimate_probability(word, list_t, n_gram, n_plus1_gram, len(vocab), k=1)\n",
    "        \n",
    "        product_pi *= 1 / probability\n",
    "\n",
    "    perplexity = product_pi**(1/float(N))\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "\n",
    "bigram = get_n_gram(train_data_processed, 2)\n",
    "trigram = get_n_gram(train_data_processed, 3)\n",
    "\n",
    "calculate_perplexity(test_data_processed[69], bigram, trigram, vocab, k=1.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
